{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from custom import utils\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses overview. \n",
    "This notebook outlines the steps for a bayesian model that attempts to find genomic information (SNPs) that are relavent for the disease (in this case schizophrenia). It does so by finding SNPs that explain the variance in a set of structural MRI thickness (sMRI) measurements well. It then finds the sMRI features which predict the diagnosis well and. Together it uses the fact that G (SNPs) -> I (sMRI) -> Y (diagnosis) to prepose that the set of SNPs (G) which explain the variance well in the set of sMRI features (I) that are important for predicting diagnosis, are important genomic locations for the disease.\n",
    "\n",
    "### Function for preprocessing\n",
    "The <b>most_handed</b> function performs categorical imputation. This decision is made based on the fact that ~90 out of 1000+ handed values are missing. That small percentage is imputed with the most frequent handedness score \"Right\". There's also a very small percentage of non right non left values that are a mix of \"Both\", \"Mixed\", \"Either\", \"Ambidextrous\", these are all imputed to just \"Both\". The other function mean centers and scales the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_handed(data):\n",
    "    \"\"\"data: pd.DataFrame\"\"\"\n",
    "    counts = Counter(data)\n",
    "    most = max(counts.items())[0]\n",
    "    data = data.copy().fillna(0)\n",
    "    data[data == 0] = 'Right'\n",
    "    both = ['Both','Mixed','Either','Ambidextrous']\n",
    "    for hand in both:\n",
    "        data[data == hand] = 'Both'\n",
    "    return data\n",
    "\n",
    "def mean_center_n_scale(data):\n",
    "    \"\"\"data: np.ndarray\"\"\"\n",
    "    data_mean_zero = data - data.mean(0)\n",
    "    data_scaled = data_mean_zero / data_mean_zero.std(0)\n",
    "    return data_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing function\n",
    "Inputs are paths to data, sMRI features and SNPs respectively. \"bcvar\" is a list of covariates: ['SEX', 'AGE_MRI', 'EstimatedTotalIntraCranialVol', 'STUDY']. \"brain_cols\" is a numpy array of feature names to subset the sMRI data. \n",
    "First thing that happens is I get a boolen array of where the brain data contains only Controls or Schizophrenics in the Group feature. Both datasets are subsetted by this boolean array, row wise. It's important to note that the datasets loaded are already in the same order rowwise. Then I create 2 sets of covariate matrices and concatenate them into one. The first set contains AGE, SEX, and ICV(EstimatedTotalIntraCranialVol), the second one is a one hot encoded matrix of handedness, the third is a one hot encoded matrix of site(i.e, study). The first step of the analysis does not use the response variable but I safe it to apply stratifiedKFold cross-validation. Two dictionaries are returned, one contains keys and numpy arrays. The name of the keys are required inputs for the first part of the analysis. The dictionary containing the column headers is not required but is saved for later uses. This function also applies mean centering and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(brain_path, snp_path, bcvar, brain_cols):\n",
    "    \"\"\"brain_path: String, snp_path: String, bcvar: list, \n",
    "    brain_cols: list or np.ndarray. \n",
    "    \"\"\"\n",
    "    # load data\n",
    "    brain_data = pd.read_hdf(brain_path)\n",
    "    snp_data = pd.read_hdf(snp_path)\n",
    "    # get the group status\n",
    "    gr = brain_data.GROUP.values\n",
    "    cnt_scz = np.logical_or(gr == 'Control', gr == 'Schizophrenia')\n",
    "    # subset by indexes cnt_scz\n",
    "    brain_data = brain_data.iloc[cnt_scz, :]\n",
    "    snp_data = snp_data.iloc[cnt_scz, :]\n",
    "    # create set of covariates\n",
    "    icv = 'EstimatedTotalIntraCranialVol'\n",
    "    cov_set1 = pd.DataFrame(\n",
    "        data=np.hstack((snp_data.SEX.values[:, None],\n",
    "                        brain_data.AGE_MRI.values[:, None],\n",
    "                        brain_data[icv].values[:, None])),\n",
    "        columns=['SEX','AGE','EstimatedTotalIntraCranialVol'])\n",
    "    cov_set1 = cov_set1.fillna(0)\n",
    "    cov_set1[cov_set1.AGE == 0] = cov_set1.AGE.mean()\n",
    "    cov_site = utils.make_non_singular(utils.encoder(brain_data.STUDY.values))\n",
    "    cov_site_cols = ['site{}'.format(i) for i in range(cov_site.shape[1])]\n",
    "    cov_site = pd.DataFrame(data=cov_site, columns=cov_site_cols)\n",
    "    cov_hand = utils.encoder(most_handed(brain_data.HANDED))\n",
    "    cov_hand_cols = ['handed{}'.format(i) for i in range(cov_hand.shape[1])]\n",
    "    cov_hand = pd.DataFrame(data=cov_hand, columns=cov_hand_cols)\n",
    "    cvars = pd.concat([cov_set1, cov_site, cov_hand], axis=1)\n",
    "    cvars_val = utils.make_non_singular(mean_center_n_scale(cvars.values))\n",
    "    y = np.array([0 if i == 'Control' else 1 for i in brain_data.GROUP.values])\n",
    "    return {'Z': cvars_val, \n",
    "            'I': mean_center_n_scale(brain_data[brain_cols].values), \n",
    "            'G': mean_center_n_scale(snp_data.iloc[:, 1:-5].values),\n",
    "            'colnames': snp_data.iloc[:, 1:-5].columns.values,\n",
    "            'y':y}, {'Z_cols': cvars.columns.values,\n",
    "                     'I_cols': brain_cols,\n",
    "                     'G_cols': snp_data.iloc[:, 1:-5].columns.values}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "These two functions assist in the analysis. save_preprocessed saves the data to disk that are outputed from the preprocess function above. The paths to where the files are written on disk are returned. This is because I'll be using nipype so to make life easier data isn't passed when interfacing with nipype nodes - just the path to where the data lives. I then load the data using the paths. The cv_maker function creates k-fold stratified cross validation indices and saves them. These indices are used in the matlab script to load the correct subsets of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_preprocessed(preproc_data_dict, preproc_data_dict_col, save_path, dn, cn):\n",
    "    \"\"\"preproc_data_dict: dictionary object returned from\n",
    "    the preprocessing function, (the first - zeroth value of the return) \n",
    "    preproc_data_dict_col: dintionary object returned from\n",
    "    the preprocessing function, (the second - first value of the return)\n",
    "    save_path: string - base path for saving the dictionaries\n",
    "    dn: string - name for saving the data dictionary \n",
    "    cn: string - name for saving the column header dictionary\n",
    "    \"\"\"\n",
    "    save_dict = os.path.join(save_path, dn)\n",
    "    save_cols = os.path.join(save_path, cn)\n",
    "    scipy.io.savemat(save_dict, mdict=preproc_data_dict)\n",
    "    utils.save_pickle(save_cols, preproc_data_dict_col)\n",
    "    return save_dict, save_cols\n",
    "\n",
    "def cv_maker(data_path, save_path):\n",
    "    \"\"\"data_path: string\n",
    "    save_path: string\n",
    "    \"\"\"\n",
    "    import scipy.io\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    X = scipy.io.loadmat(data_path)['I']\n",
    "    y = scipy.io.loadmat(data_path)['y'][0]\n",
    "    cv = StratifiedKFold(n_splits=5, random_state=1)\n",
    "    train_idx, test_idx = {}, {}\n",
    "    for idx, (train, test) in enumerate(cv.split(X, y)):\n",
    "        train_idx['train_{}'.format(idx + 1)] = train + 1\n",
    "        test_idx['test_{}'.format(idx + 1)] = test + 1\n",
    "    scipy.io.savemat(save_path, mdict={\"train\":train_idx, \"test\":test_idx})\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the input data, save the CV indices\n",
    "The step below runs the functions I've made above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers_dir = \"/storage/gablab001/data/genus/GIT/genus/fs_cog/pred_diag/column_headers\"\n",
    "brain_cols = np.genfromtxt(os.path.join(headers_dir, \"XB\"), dtype=str)\n",
    "brain_path = \"/storage/gablab001/data/genus/GIT/genus/bayes/data_sets/brain_N1547_P5927_matched.hdf5py\"\n",
    "snp_path = \"/storage/gablab001/data/genus/GIT/genus/bayes/data_sets/genomic_N1547_P100006_matched.hdf5py\"\n",
    "bcv = ['SEX', 'AGE_MRI', 'EstimatedTotalIntraCranialVol', 'STUDY']\n",
    "all_data, all_cols = preprocess(brain_path, snp_path, bcv, brain_cols)\n",
    "path_for_save = \"/storage/gablab001/data/genus/GIT/genus/bayes/data_sets\"\n",
    "#for_cv, _ = save_preprocessed(all_data, all_cols, path_for_save, \"brain_gene.mat\",\"brain_gene_cols.pkl\")\n",
    "#cv_path = cv_maker(for_cv, os.path.join(path_for_save, \"cv_idx.mat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nipype import Function, Node, Workflow, IdentityInterface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian analysis - \"step 1\" \n",
    "This step attempts to find a subset of SNPs per each feature in the sMRI data that explain the variance well in that feature. Below I create the workflow that I use with nipype, create the nipype wrapper nodes to wrap functions that will go into the nipype graph, and then submit the jobs. Due to the nature of the analysis we are parallelizing over the feature space in the sMRI data. That is - one job per feature, on top of that we are parallelizing the cross validation step. In total this means there are (170*10) jobs that need to be submitted. For a single user in my experience that's too many jobs for the Openmind cluster so I limit the amount of jobs that can be submitted at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wf_bf = Workflow(name='brain_bcv')\n",
    "wf_bf.base_dir = \"/om/scratch/Tue/ysa\"\n",
    "\n",
    "Iternode = Node(IdentityInterface(fields=['col_idx', 'cv_idx']), name = 'Iternode')\n",
    "Iternode.iterables = [('col_idx', np.arange(170) + 1), ('cv_idx', np.arange(5) + 1)]\n",
    "\n",
    "def run_bayes(in_file, cv_file, cv_idx, col_idx, out_file):\n",
    "    \"\"\"in_file: string - path to base data file\n",
    "    cv_file: string - path to cross validation indices stored in a .mat file\n",
    "    cv_idx: int - ranges from 1 to K+1, K = number of CV folds\n",
    "    col_idx: int - ranges from 1 to P+1, P = number of columns in sMRI features\n",
    "    out_file: string - path to where the output is saved\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import cPickle as pickle\n",
    "    import nipype.interfaces.matlab as Matlab\n",
    "    def outnames(col, out):\n",
    "        return os.path.join(out, '{}.mat'.format(col))\n",
    "    headers_dir = \"/storage/gablab001/data/genus/GIT/genus/fs_cog/pred_diag/column_headers\"\n",
    "    col_names = np.genfromtxt(os.path.join(headers_dir, \"XB\"), dtype=str)\n",
    "    col_save_name = col_names[col_idx - 1] + \"_{}_{}_BF\".format(cv_idx, col_idx)\n",
    "    with open(\"/storage/gablab001/data/genus/GIT/genus/bayes/matlab/bayes_reg.m\", \"r\") as src:\n",
    "        script = src.read().replace(\"\\n\", \"\")\n",
    "    mat_file = outnames(in_file[:-4]+'_'+col_save_name, out_file)\n",
    "    matlab = Matlab.MatlabCommand()\n",
    "    matlab.inputs.script = script.format(in_file, cv_file, cv_idx, col_idx, mat_file)\n",
    "    res = matlab.run()\n",
    "    return mat_file\n",
    "\n",
    "Run_bayes = Node(interface=Function(\n",
    "    input_names = ['in_file', 'cv_file','cv_idx',\n",
    "                   'col_idx','out_file'],\n",
    "    output_names = ['mat_file'],\n",
    "    function = run_bayes\n",
    "), name='Run_bayes')\n",
    "\n",
    "Run_bayes.inputs.in_file = \"/storage/gablab001/data/genus/GIT/genus/bayes/data_sets/brain_gene.mat\"\n",
    "Run_bayes.inputs.cv_file = \"/storage/gablab001/data/genus/GIT/genus/bayes/data_sets/cv_idx.mat\"\n",
    "Run_bayes.inputs.out_file = \"/storage/gablab001/data/genus/GIT/genus/bayes/results/bayes_factor\"\n",
    "wf_bf.connect(Iternode, 'cv_idx', Run_bayes, 'cv_idx')\n",
    "wf_bf.connect(Iternode, 'col_idx', Run_bayes, 'col_idx')\n",
    "#wf.run(plugin='SLURM', plugin_args={'sbatch_args':'--mem=4G -t 23:00:00', 'max_jobs': 170})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up for step 2\n",
    "Step 1 above uses an updated version of varbvs and in <b>bayes_reg.m</b> the normalization step is already performed. The matlab file <b>deployendophenVB.m</b> wants a csv file for step 2 that points to the output from step 1. The functions below create one of those for each fold. So 5 folds, 5 files where each file has 170 paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bayes_factor_outputs = \"/storage/gablab001/data/genus/GIT/genus/bayes/results/bayes_factor/5foldcv_allcov\"\n",
    "bf_res = [os.path.join(bayes_factor_outputs,x) for x in os.listdir(bayes_factor_outputs)]\n",
    "\n",
    "def make_file(paths):\n",
    "    cv = [i.split('_')[-3] for i in paths]\n",
    "    df = pd.DataFrame({'path':paths, 'cv':cv})\n",
    "    groups = df.groupby('cv')\n",
    "    df_store = []\n",
    "    for group in groups.groups:\n",
    "        gr = groups.get_group(group).reset_index(drop=True).drop('cv', 1)\n",
    "        gr['sorter'] = [int(i.split('_')[-2]) for i in gr.path]\n",
    "        gr = gr.sort_values('sorter').reset_index(drop=True)\n",
    "        gr.columns=['matFn','colNum']\n",
    "        df_store.append(gr[['colNum', 'matFn']])\n",
    "    return df_store\n",
    " \n",
    "#for idx, f in enumerate(make_file(bf_res)):\n",
    "#    f.to_csv('BFRESULT_CV_{}.csv'.format(idx+1), index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed form variational bayes \"Step 2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170728-11:45:31,213 workflow INFO:\n",
      "\t Workflow fxvb_step settings: ['check', 'execution', 'logging']\n",
      "170728-11:45:31,235 workflow INFO:\n",
      "\t Running in parallel.\n",
      "170728-11:45:31,239 workflow INFO:\n",
      "\t Pending[0] Submitting[5] jobs Slots[20]\n",
      "170728-11:45:31,240 workflow INFO:\n",
      "\t Submitting: Run_fxvb.a3 ID: 0\n",
      "170728-11:45:31,311 workflow INFO:\n",
      "\t Finished submitting: Run_fxvb.a3 ID: 0\n",
      "170728-11:45:31,312 workflow INFO:\n",
      "\t Submitting: Run_fxvb.a2 ID: 1\n",
      "170728-11:45:31,381 workflow INFO:\n",
      "\t Finished submitting: Run_fxvb.a2 ID: 1\n",
      "170728-11:45:31,382 workflow INFO:\n",
      "\t Submitting: Run_fxvb.a4 ID: 2\n",
      "170728-11:45:31,451 workflow INFO:\n",
      "\t Finished submitting: Run_fxvb.a4 ID: 2\n",
      "170728-11:45:31,453 workflow INFO:\n",
      "\t Submitting: Run_fxvb.a1 ID: 3\n",
      "170728-11:45:31,519 workflow INFO:\n",
      "\t Finished submitting: Run_fxvb.a1 ID: 3\n",
      "170728-11:45:31,521 workflow INFO:\n",
      "\t Submitting: Run_fxvb.a0 ID: 4\n",
      "170728-11:45:31,585 workflow INFO:\n",
      "\t Finished submitting: Run_fxvb.a0 ID: 4\n"
     ]
    }
   ],
   "source": [
    "wf_fxvb = Workflow(name='fxvb_step')\n",
    "wf_fxvb.base_dir = '/om/scratch/Fri/ysa'\n",
    "\n",
    "def run_fxvb(in_file, csv_file, out_file):\n",
    "    \"\"\"in_file: string - path to base input data file\n",
    "    csv_file: string - path to a template of the csv_file created \n",
    "              by \"make_file\" func\n",
    "    out_file: string - path for the output of run_fxvb function\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import nipype.interfaces.matlab as Matlab\n",
    "    with open(\"/storage/gablab001/data/genus/GIT/genus/bayes/matlab/fxvb.m\", \"r\") as src:\n",
    "        script = src.read().replace(\"\\n\", \"\")\n",
    "    mat_name = 'fxvb_'+csv_file.split('/')[-1][-8:-4]+'.mat'\n",
    "    mat_file = os.path.join(out_file, mat_name)\n",
    "    matlab = Matlab.MatlabCommand()\n",
    "    matlab.inputs.script = script.format(csv_file, in_file, mat_file)\n",
    "    print(script.format(csv_file, in_file, mat_file))\n",
    "    res = matlab.run()  \n",
    "    return mat_file\n",
    "    \n",
    "Run_fxvb = Node(interface=Function(\n",
    "    input_names = ['in_file', 'csv_file','out_file'],\n",
    "    output_names = ['mat_file'],\n",
    "    function = run_fxvb\n",
    "), name='Run_fxvb')\n",
    "\n",
    "cvpath = \"/storage/gablab001/data/genus/GIT/genus/bayes/results/bayes_factor\"\n",
    "IterCV = Node(IdentityInterface(fields=['csv_file']), name = 'Iternode')\n",
    "IterCV.iterables = ('csv_file', [os.path.join(cvpath, x) for x in os.listdir(cvpath) if \"_CV_\" in x])\n",
    "Run_fxvb.inputs.in_file = '/storage/gablab001/data/genus/GIT/genus/bayes/data_sets/brain_gene.mat'\n",
    "Run_fxvb.inputs.out_file = '/storage/gablab001/data/genus/GIT/genus/bayes/results/fxvb'\n",
    "wf_fxvb.connect(IterCV, 'csv_file', Run_fxvb, 'csv_file')\n",
    "wf_fxvb.run(plugin='SLURM', plugin_args={'sbatch_args':'--mem=10G -t 4-23:00:00'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single runs because slurm is just weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nipype.interfaces.matlab as Matlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/storage/gablab001/data/genus/GIT/genus/bayes/matlab/fxvb.m\", \"r\") as src:\n",
    "        script = src.read().replace(\"\\n\", \"\")\n",
    "csv_file = \"/storage/gablab001/data/genus/GIT/genus/bayes/results/bayes_factor/BFRESULT_CV_1.csv\"\n",
    "in_file = \"/storage/gablab001/data/genus/GIT/genus/bayes/data_sets/brain_gene.mat\"\n",
    "out_file = \"/storage/gablab001/data/genus/GIT/genus/bayes/results/fxvb\"\n",
    "mat_name = 'fxvb_'+csv_file.split('/')[-1][-8:-4]+'.mat'\n",
    "mat_file = os.path.join(out_file, mat_name)\n",
    "matlab = Matlab.MatlabCommand()\n",
    "matlab.inputs.script = script.format(csv_file, in_file, mat_file)\n",
    "res = matlab.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
